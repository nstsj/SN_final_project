{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "paramiko missing, opening SSH/SCP/SFTP paths will be disabled.  `pip install paramiko` to suppress\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "import json\n",
    "import re\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "from random import choice\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras import backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "\n",
    "    Only computes a batch-wise average of recall.\n",
    "\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "\n",
    "    Only computes a batch-wise average of precision.\n",
    "\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def authors_to_emb(authors, embs):\n",
    "    try:\n",
    "        out = list(embs[authors[0]])\n",
    "        out.extend(list(embs[authors[1]]))\n",
    "        return out\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "\n",
    "def all_coauthors_pairs(coauthors, exception_graph=None):\n",
    "    if len(coauthors) < 2:\n",
    "        return None\n",
    "    pairs = []\n",
    "    for i in range(len(coauthors)):\n",
    "        for j in range(len(coauthors[i + 1:])):\n",
    "            if exception_graph:\n",
    "                try:\n",
    "                    dist = nx.shortest_path_length(graph, coauthors[i], coauthors[i + j + 1])\n",
    "                except:\n",
    "                    dist = 100\n",
    "                if dist != 1:\n",
    "                    pairs.append([coauthors[i], coauthors[i + j + 1]])\n",
    "            else:\n",
    "                pairs.append([coauthors[i], coauthors[i + j + 1]])\n",
    "    return pairs\n",
    "\n",
    "\n",
    "def random_not_coauthors(graph, need_num, authors):\n",
    "    pairs = []\n",
    "    for _ in range(need_num * 4):\n",
    "        a1 = choice(list(graph.nodes()))\n",
    "        a2 = choice(list(graph.nodes()))\n",
    "        \n",
    "        try:\n",
    "            dist = nx.shortest_path_length(graph, a1, a2)\n",
    "        except:\n",
    "            dist = 100\n",
    "        if (not a1 == a2) and (dist > 1) and (a1 in authors) and (a2 in authors):\n",
    "            pairs.append([a1, a2])\n",
    "        if len(pairs) >= need_num:\n",
    "            break\n",
    "    return pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('arxivData.json') as json_data:\n",
    "    dataset = json.load(json_data)\n",
    "    json_data.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph = nx.Graph()\n",
    "authors = set([])\n",
    "X_train_val = []\n",
    "y_train_val = []\n",
    "for paper in dataset:\n",
    "    if 2014 <= paper['year'] <= 2016:\n",
    "        try:\n",
    "            paper_authors = []\n",
    "            for author in json.loads(re.sub(\"'\", '\"', paper['author'])):\n",
    "                paper_authors.append(author['name'])\n",
    "            if len(paper_authors) > 1:\n",
    "                authors.update(paper_authors)\n",
    "            coauthors = all_coauthors_pairs(paper_authors)\n",
    "            for pair in coauthors:\n",
    "                graph.add_edge(pair[0], pair[1])\n",
    "            X_train_val.extend(coauthors)\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "data_size = len(X_train_val)\n",
    "X_train_val.extend(random_not_coauthors(graph, data_size * 1, authors))\n",
    "\n",
    "y_train_val = [[0.0, 1.0]] * data_size\n",
    "y_train_val.extend([[1.0, 0.0]] * (len(X_train_val) - data_size))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "graph_test = nx.Graph()\n",
    "X_test = []\n",
    "y_test = []\n",
    "for paper in dataset:\n",
    "    if paper['year'] >= 2017:\n",
    "        try:\n",
    "            paper_authors = []\n",
    "            for author in json.loads(re.sub(\"'\", '\"', paper['author'])):\n",
    "                if author['name'] in authors:\n",
    "                    paper_authors.append(author['name'])\n",
    "            coauthors = all_coauthors_pairs(paper_authors)\n",
    "            for pair in coauthors:\n",
    "                graph_test.add_edge(pair[0], pair[1])\n",
    "            X_test.extend(all_coauthors_pairs(paper_authors, graph))\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "\n",
    "data_size = len(X_test)\n",
    "X_test.extend(random_not_coauthors(graph_test, data_size*5, authors))\n",
    "\n",
    "y_test = [[0.0, 1.0]] * data_size\n",
    "y_test.extend([[1.0, 0.0]] * (len(X_test) - data_size))\n",
    "\n",
    "X_test_raw = X_test\n",
    "y_test_raw = y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask_valid = [(pair[0] in authors and pair[1] in authors) for pair in X_test_raw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_valid = list(np.array(X_test_raw)[mask_valid])\n",
    "y_test_valid = list(np.array(y_test_raw)[mask_valid])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.83      1.00      0.91     69435\n",
      "           1       0.96      0.00      0.01     13887\n",
      "\n",
      "   micro avg       0.83      0.83      0.83     83322\n",
      "   macro avg       0.90      0.50      0.46     83322\n",
      "weighted avg       0.86      0.83      0.76     83322\n",
      "\n",
      "\n",
      "\n",
      "[[69433     2]\n",
      " [13832    55]]\n",
      "0.5989206589127878\n"
     ]
    }
   ],
   "source": [
    "preds_nx = [pred for _, _, pred in nx.jaccard_coefficient(graph, [x for x in X_test_raw])]\n",
    "\n",
    "y_test = [np.argmax(y) for y in np.array(y_test_raw)]\n",
    "\n",
    "y_pred = [1 if pred >= 0.5 else 0 for pred in preds_nx]\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('\\n')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, preds_nx))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████| 24475/24475 [00:08<00:00, 2930.96it/s]\n"
     ]
    }
   ],
   "source": [
    "# print('Generate walks')\n",
    "node2vec = Node2Vec(graph, dimensions=20, walk_length=16, num_walks=10, workers=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learn embeddings\n"
     ]
    }
   ],
   "source": [
    "print('Learn embeddings')\n",
    "embs = node2vec.fit(window=10, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Dense(40))\n",
    "model.add(Dense(30, activation='tanh'))\n",
    "model.add(Dense(20, activation='relu'))\n",
    "# model.add(Dense(10, activation='exponential'))  ### Вот этот слой лишний. Возможно активация не очень.\n",
    "model.add(Dense(2, activation='softmax'))\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer='rmsprop', metrics=[\"accuracy\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:32: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 126310 samples, validate on 31578 samples\n",
      "Epoch 1/40\n",
      "126310/126310 [==============================] - 1s 10us/step - loss: 0.2595 - acc: 0.8954 - val_loss: 0.1410 - val_acc: 0.9492\n",
      "Epoch 2/40\n",
      "126310/126310 [==============================] - 1s 8us/step - loss: 0.1187 - acc: 0.9564 - val_loss: 0.1056 - val_acc: 0.9612\n",
      "Epoch 3/40\n",
      "126310/126310 [==============================] - 1s 8us/step - loss: 0.0899 - acc: 0.9680 - val_loss: 0.0850 - val_acc: 0.9692\n",
      "Epoch 4/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0758 - acc: 0.9729 - val_loss: 0.0797 - val_acc: 0.9720\n",
      "Epoch 5/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0673 - acc: 0.9762 - val_loss: 0.0821 - val_acc: 0.9704\n",
      "Epoch 6/40\n",
      "126310/126310 [==============================] - 1s 8us/step - loss: 0.0613 - acc: 0.9782 - val_loss: 0.0707 - val_acc: 0.9765\n",
      "Epoch 7/40\n",
      "126310/126310 [==============================] - 1s 8us/step - loss: 0.0579 - acc: 0.9796 - val_loss: 0.0648 - val_acc: 0.9784\n",
      "Epoch 8/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0534 - acc: 0.9813 - val_loss: 0.0700 - val_acc: 0.9781\n",
      "Epoch 9/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0506 - acc: 0.9822 - val_loss: 0.0619 - val_acc: 0.9798\n",
      "Epoch 10/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0479 - acc: 0.9830 - val_loss: 0.0568 - val_acc: 0.9813\n",
      "Epoch 11/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0458 - acc: 0.9841 - val_loss: 0.0621 - val_acc: 0.9790\n",
      "Epoch 12/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0446 - acc: 0.9848 - val_loss: 0.0599 - val_acc: 0.9799\n",
      "Epoch 13/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0428 - acc: 0.9856 - val_loss: 0.0611 - val_acc: 0.9800\n",
      "Epoch 14/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0420 - acc: 0.9858 - val_loss: 0.0519 - val_acc: 0.9822\n",
      "Epoch 15/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0406 - acc: 0.9862 - val_loss: 0.0487 - val_acc: 0.9839\n",
      "Epoch 16/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0393 - acc: 0.9867 - val_loss: 0.0494 - val_acc: 0.9840\n",
      "Epoch 17/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0384 - acc: 0.9867 - val_loss: 0.0503 - val_acc: 0.9834\n",
      "Epoch 18/40\n",
      "126310/126310 [==============================] - 1s 10us/step - loss: 0.0376 - acc: 0.9874 - val_loss: 0.0487 - val_acc: 0.9844\n",
      "Epoch 19/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0364 - acc: 0.9876 - val_loss: 0.0487 - val_acc: 0.9838\n",
      "Epoch 20/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0356 - acc: 0.9881 - val_loss: 0.0534 - val_acc: 0.9835\n",
      "Epoch 21/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0350 - acc: 0.9882 - val_loss: 0.0561 - val_acc: 0.9812\n",
      "Epoch 22/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0342 - acc: 0.9884 - val_loss: 0.0497 - val_acc: 0.9839\n",
      "Epoch 23/40\n",
      "126310/126310 [==============================] - 1s 10us/step - loss: 0.0330 - acc: 0.9887 - val_loss: 0.0470 - val_acc: 0.9851\n",
      "Epoch 24/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0330 - acc: 0.9886 - val_loss: 0.0435 - val_acc: 0.9862\n",
      "Epoch 25/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0321 - acc: 0.9893 - val_loss: 0.0457 - val_acc: 0.9854\n",
      "Epoch 26/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0315 - acc: 0.9894 - val_loss: 0.0462 - val_acc: 0.9851\n",
      "Epoch 27/40\n",
      "126310/126310 [==============================] - 1s 10us/step - loss: 0.0311 - acc: 0.9894 - val_loss: 0.0471 - val_acc: 0.9849\n",
      "Epoch 28/40\n",
      "126310/126310 [==============================] - 1s 10us/step - loss: 0.0310 - acc: 0.9894 - val_loss: 0.0427 - val_acc: 0.9864\n",
      "Epoch 29/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0306 - acc: 0.9895 - val_loss: 0.0483 - val_acc: 0.9859\n",
      "Epoch 30/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0294 - acc: 0.9901 - val_loss: 0.0463 - val_acc: 0.9852\n",
      "Epoch 31/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0299 - acc: 0.9897 - val_loss: 0.0445 - val_acc: 0.9857\n",
      "Epoch 32/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0298 - acc: 0.9900 - val_loss: 0.0488 - val_acc: 0.9843\n",
      "Epoch 33/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0289 - acc: 0.9902 - val_loss: 0.0456 - val_acc: 0.9860\n",
      "Epoch 34/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0288 - acc: 0.9903 - val_loss: 0.0566 - val_acc: 0.9834\n",
      "Epoch 35/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0293 - acc: 0.9898 - val_loss: 0.0436 - val_acc: 0.9870\n",
      "Epoch 36/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0290 - acc: 0.9901 - val_loss: 0.0443 - val_acc: 0.9858\n",
      "Epoch 37/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0290 - acc: 0.9901 - val_loss: 0.0464 - val_acc: 0.9854\n",
      "Epoch 38/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0287 - acc: 0.9900 - val_loss: 0.0418 - val_acc: 0.9870\n",
      "Epoch 39/40\n",
      "126310/126310 [==============================] - 1s 8us/step - loss: 0.0282 - acc: 0.9904 - val_loss: 0.0441 - val_acc: 0.9862\n",
      "Epoch 40/40\n",
      "126310/126310 [==============================] - 1s 9us/step - loss: 0.0279 - acc: 0.9908 - val_loss: 0.0433 - val_acc: 0.9862\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x13f3915c0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "X_train = np.array([authors_to_emb(pair, embs) for pair in X_train])\n",
    "y_train = np.array(y_train)\n",
    "X_val = np.array([authors_to_emb(pair, embs) for pair in X_val])\n",
    "y_val = np.array(y_val)\n",
    "model.fit(X_train, y_train, batch_size=128, epochs=40, validation_data=(X_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     15866\n",
      "           1       0.98      0.99      0.99     15712\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     31578\n",
      "   macro avg       0.99      0.99      0.99     31578\n",
      "weighted avg       0.99      0.99      0.99     31578\n",
      "\n",
      "\n",
      "\n",
      "[[15629   237]\n",
      " [  200 15512]]\n",
      "0.9985027694550053\n"
     ]
    }
   ],
   "source": [
    "# 1 - соавторы, 0 - нет\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.99      0.99      0.99     15866\n",
    "#            1       0.99      0.99      0.99     15712\n",
    "\n",
    "#    micro avg       0.99      0.99      0.99     31578\n",
    "#    macro avg       0.99      0.99      0.99     31578\n",
    "# weighted avg       0.99      0.99      0.99     31578\n",
    "# [[15652   214]\n",
    "#  [  155 15557]]\n",
    "# 0.9989353879890981\n",
    "\n",
    "\n",
    "y_pred = [np.argmax(y) for y in model.predict(X_val)]\n",
    "y_true = [np.argmax(y) for y in y_val]\n",
    "print(classification_report(y_true, y_pred))\n",
    "print('\\n')\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(roc_auc_score(y_true, [proba[1] for proba in model.predict(X_val)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:31: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n",
      "/usr/local/lib/python3.7/site-packages/ipykernel_launcher.py:32: DeprecationWarning: Call to deprecated `__getitem__` (Method will be removed in 4.0.0, use self.wv.__getitem__() instead).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.87      0.97      0.91     69435\n",
      "           1       0.61      0.27      0.37     13887\n",
      "\n",
      "   micro avg       0.85      0.85      0.85     83322\n",
      "   macro avg       0.74      0.62      0.64     83322\n",
      "weighted avg       0.82      0.85      0.82     83322\n",
      "\n",
      "\n",
      "\n",
      "[[67018  2417]\n",
      " [10184  3703]]\n",
      "0.6937477277855997\n"
     ]
    }
   ],
   "source": [
    "# nx.resource_allocation_index\n",
    "#               precision    recall  f1-score   support\n",
    "#            0       0.83      1.00      0.91     69435\n",
    "#            1       0.76      0.00      0.00     13887\n",
    "\n",
    "#    micro avg       0.83      0.83      0.83     83322\n",
    "#    macro avg       0.80      0.50      0.46     83322\n",
    "# weighted avg       0.82      0.83      0.76     83322\n",
    "# [[69429     6]\n",
    "#  [13868    19]]\n",
    "# 0.5990594303456509\n",
    "\n",
    "# nx.jaccard_coefficient \n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.83      1.00      0.91     69435\n",
    "#            1       1.00      0.00      0.01     13887\n",
    "\n",
    "#    micro avg       0.83      0.83      0.83     83322\n",
    "#    macro avg       0.92      0.50      0.46     83322\n",
    "# weighted avg       0.86      0.83      0.76     83322\n",
    "# [[69435     0]\n",
    "#  [13832    55]]\n",
    "# 0.5990585638635837\n",
    "\n",
    "\n",
    "# 1*1 Logreg\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.86      0.99      0.92     69353\n",
    "#            1       0.83      0.19      0.31     13887\n",
    "\n",
    "#    micro avg       0.86      0.86      0.86     83240\n",
    "#    macro avg       0.84      0.59      0.62     83240\n",
    "# weighted avg       0.85      0.86      0.82     83240\n",
    "# [[68812   541]\n",
    "#  [11241  2646]]\n",
    "# 0.6395078283412827\n",
    "\n",
    "# 5*1 All_pairs\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.92      0.98      0.95    135084\n",
    "#            1       0.84      0.59      0.69     27216\n",
    "\n",
    "#    micro avg       0.91      0.91      0.91    162300\n",
    "#    macro avg       0.88      0.78      0.82    162300\n",
    "# weighted avg       0.91      0.91      0.91    162300\n",
    "# [[132044   3040]\n",
    "#  [ 11130  16086]]\n",
    "\n",
    "# 5*1 New pairs\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.87      0.98      0.92     69367\n",
    "#            1       0.68      0.24      0.35     13887\n",
    "\n",
    "#    micro avg       0.85      0.85      0.85     83254\n",
    "#    macro avg       0.77      0.61      0.64     83254\n",
    "# weighted avg       0.83      0.85      0.82     83254\n",
    "# [[67779  1588]\n",
    "#  [10569  3318]]\n",
    "# 0.6553634735001194\n",
    "\n",
    "# Random\n",
    "#              precision    recall  f1-score   support\n",
    "\n",
    "#            0       1.00      0.79      0.88     83254\n",
    "#            1       0.00      0.00      0.00         0\n",
    "\n",
    "#    micro avg       0.79      0.79      0.79     83254\n",
    "#    macro avg       0.50      0.39      0.44     83254\n",
    "# weighted avg       1.00      0.79      0.88     83254\n",
    "# [[65546 17708]\n",
    "#  [    0     0]]\n",
    "\n",
    "# 1*1 New pairs\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.87      0.95      0.91     69353\n",
    "#            1       0.55      0.29      0.38     13887\n",
    "\n",
    "#    micro avg       0.84      0.84      0.84     83240\n",
    "#    macro avg       0.71      0.62      0.65     83240\n",
    "# weighted avg       0.82      0.84      0.82     83240\n",
    "# [[66055  3298]\n",
    "#  [ 9823  4064]]\n",
    "# 0.6657878721401573\n",
    "\n",
    "# 1*1 New pairs Stack more layers 40e\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.87      0.96      0.91     69341\n",
    "#            1       0.59      0.30      0.39     13887\n",
    "\n",
    "#    micro avg       0.85      0.85      0.85     83228\n",
    "#    macro avg       0.73      0.63      0.65     83228\n",
    "# weighted avg       0.82      0.85      0.83     83228\n",
    "# [[66511  2830]\n",
    "#  [ 9789  4098]]\n",
    "# 0.7027983388267736\n",
    "\n",
    "# 1*1 New pairs Stack MORE layers 40e\n",
    "#               precision    recall  f1-score   support\n",
    "\n",
    "#            0       0.87      0.97      0.92     69341\n",
    "#            1       0.64      0.27      0.38     13887\n",
    "\n",
    "#    micro avg       0.85      0.85      0.85     83228\n",
    "#    macro avg       0.75      0.62      0.65     83228\n",
    "# weighted avg       0.83      0.85      0.83     83228\n",
    "# [[67246  2095]\n",
    "#  [10151  3736]]\n",
    "# 0.6888330186522706\n",
    "\n",
    "mask_valid = [np.array(authors_to_emb(pair, embs)).shape == (40,) for pair in X_test_raw]\n",
    "X_test = np.array([authors_to_emb(pair, embs) for pair in X_test_raw\n",
    "                   if np.array(authors_to_emb(pair, embs)).shape == (40,)])\n",
    "y_test = np.array(y_test_raw)[mask_valid]\n",
    "y_pred = [np.argmax(y) for y in model.predict(X_test)]\n",
    "y_true = [np.argmax(y) for y in y_test]\n",
    "print(classification_report(y_true, y_pred))\n",
    "print('\\n')\n",
    "print(confusion_matrix(y_true, y_pred))\n",
    "print(roc_auc_score(y_true, [proba[1] for proba in model.predict(X_test)]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size=0.2, random_state=42)\n",
    "X_train = np.array([cosine_similarity(embs.wv.__getitem__(pair[0]).reshape(1, -1),\n",
    "                                      embs.wv.__getitem__(pair[1]).reshape(1, -1))[0] for pair in X_train])\n",
    "y_train = [np.argmax(y) for y in y_train]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_reg = LogisticRegression()\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.99      0.99     15866\n",
      "           1       0.99      0.99      0.99     15712\n",
      "\n",
      "   micro avg       0.99      0.99      0.99     31578\n",
      "   macro avg       0.99      0.99      0.99     31578\n",
      "weighted avg       0.99      0.99      0.99     31578\n",
      "\n",
      "\n",
      "\n",
      "[[15751   115]\n",
      " [   80 15632]]\n"
     ]
    }
   ],
   "source": [
    "X_val = np.array([cosine_similarity(embs.wv.__getitem__(pair[0]).reshape(1, -1),\n",
    "                                      embs.wv.__getitem__(pair[1]).reshape(1, -1))[0] for pair in X_val])\n",
    "y_val = [np.argmax(y) for y in y_val]\n",
    "\n",
    "y_pred = log_reg.predict(X_val)\n",
    "print(classification_report(y_val, y_pred))\n",
    "print('\\n')\n",
    "print(confusion_matrix(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.99      0.92     69435\n",
      "           1       0.84      0.18      0.30     13887\n",
      "\n",
      "   micro avg       0.86      0.86      0.86     83322\n",
      "   macro avg       0.85      0.59      0.61     83322\n",
      "weighted avg       0.86      0.86      0.82     83322\n",
      "\n",
      "\n",
      "\n",
      "[[68960   475]\n",
      " [11325  2562]]\n",
      "0.6460888759938105\n"
     ]
    }
   ],
   "source": [
    "X_test = np.array([cosine_similarity(embs.wv.__getitem__(pair[0]).reshape(1, -1),\n",
    "                                      embs.wv.__getitem__(pair[1]).reshape(1, -1))[0] \n",
    "                   for pair in np.array(X_test_raw)[mask_valid]])\n",
    "y_test = [np.argmax(y) for y in np.array(y_test_raw)[mask_valid]]\n",
    "\n",
    "y_pred = log_reg.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "print('\\n')\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(roc_auc_score(y_test, [proba[1] for proba in log_reg.predict_proba(X_test)]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
